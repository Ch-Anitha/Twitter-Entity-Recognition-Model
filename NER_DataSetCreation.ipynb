{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_file_path.txt' with the correct path and name of your text file\n",
        "with open('test.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Extract words and labels from each line, considering non-empty lines\n",
        "words = []\n",
        "labels = []\n",
        "\n",
        "for line in lines:\n",
        "    parts = line.split()\n",
        "    if len(parts) >= 2:\n",
        "        words.append(parts[0])\n",
        "        labels.append(parts[1])\n",
        "\n",
        "# Create a DataFrame with two columns\n",
        "df = pd.DataFrame({'Word': words, 'Label': labels})\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df.to_csv('Book.csv', index=False)\n"
      ],
      "metadata": {
        "id": "0TIKUcu5BoN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_dataset.txt' with the correct file path and name\n",
        "with open('test.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Initialize lists to store sentences and tags\n",
        "sentences = []\n",
        "tags = []\n",
        "\n",
        "# Process lines to group words from the same sentence together\n",
        "current_sentence = []\n",
        "current_tags = []\n",
        "\n",
        "for line in lines:\n",
        "    # Split each line into word and tag\n",
        "    parts = line.strip().split('\\t')\n",
        "\n",
        "    # Check if the line contains both a word and a tag\n",
        "    if len(parts) == 2:\n",
        "        word, tag = parts\n",
        "    else:\n",
        "        # Handle lines without the expected format\n",
        "        continue\n",
        "\n",
        "    # Check for the end of a sentence\n",
        "    if word == '.':\n",
        "        current_sentence.append(word)\n",
        "        current_tags.append(tag)\n",
        "        sentences.append(' '.join(current_sentence))\n",
        "        tags.append(' '.join(current_tags))\n",
        "        current_sentence = []\n",
        "        current_tags = []\n",
        "    else:\n",
        "        current_sentence.append(word)\n",
        "        current_tags.append(tag)\n",
        "\n",
        "# Create a DataFrame with sentences and tags\n",
        "df = pd.DataFrame({'sentence': sentences, 'tags': tags})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('Book.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "Cyq29GBWILLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "with open('test.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "sentences = []\n",
        "tags = []\n",
        "current_sentence = []\n",
        "current_tags = []\n",
        "\n",
        "for line in lines:\n",
        "    # Spliting each line into word and tag\n",
        "    parts = line.strip().split('\\t')\n",
        "\n",
        "    # Check if the line contains both a word and a tag\n",
        "    if len(parts) == 2:\n",
        "        word, tag = parts\n",
        "    else:\n",
        "        # Handle lines without the expected format\n",
        "        continue\n",
        "\n",
        "    # Check for the end of a sentence using spaCy\n",
        "    current_sentence.append(word)\n",
        "    current_tags.append(tag)\n",
        "\n",
        "    if nlp(word).sentiment < 0 and word.endswith('?') or word.endswith('!') or word.endswith('.'):\n",
        "        # End of a sentence detected\n",
        "        sentences.append(' '.join(current_sentence))\n",
        "        tags.append(' '.join(current_tags))\n",
        "        current_sentence = []\n",
        "        current_tags = []\n",
        "\n",
        "# Create a DataFrame with sentences and tags\n",
        "df = pd.DataFrame({'sentence': sentences, 'tags': tags})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('Book.csv', index=False)\n"
      ],
      "metadata": {
        "id": "3qq7qUWlJIfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to remove all keys except tokens and tags"
      ],
      "metadata": {
        "id": "uD2wPVoU0a85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('dataset_2020.train.json', 'r') as file:\n",
        "    readfile = file.readlines()\n",
        "\n",
        "# selecting each line in json file\n",
        "datalist = []\n",
        "for line in readfile:\n",
        "    datalist.append(json.loads(line))\n",
        "\n",
        "# If the dictionary has keys tokens and tags then they are returned to add in the array\n",
        "def modifyrows(row, keystokeep):\n",
        "    modifiedvalues = {}\n",
        "    for key, value in row.items():\n",
        "        if key in keystokeep: # If the key is either token or its tag\n",
        "            modifiedvalues[key] = value # add both the token and tag to the array\n",
        "    return modifiedvalues\n",
        "\n",
        "#selecting the keys we want in the dataset\n",
        "keystokeep = [\"tokens\", \"tags\"]\n",
        "\n",
        "#updating each row and adding it to the array\n",
        "updateddata = []\n",
        "for row in datalist:\n",
        "    updateddata.append(modifyrows(row, keystokeep))\n",
        "\n",
        "#adding the updated rows to file\n",
        "updatedfile = ''\n",
        "for item in updateddata:\n",
        "    updatedfile += json.dumps(item) + '\\n'\n",
        "\n",
        "with open('dataset_2020.train.json', 'w') as file:\n",
        "    file.write(updatedfile)\n"
      ],
      "metadata": {
        "id": "2xOO8Lifl3lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to replace the tokens"
      ],
      "metadata": {
        "id": "dQzyQrjL0xQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Read the data from the input file\n",
        "with open('dataset_2020.train.json', 'r') as file:\n",
        "    data_lines = file.readlines()\n",
        "\n",
        "# Parse the JSON data\n",
        "data_list = [json.loads(line) for line in data_lines]\n",
        "\n",
        "# Function to replace tag numbers with specified mappings\n",
        "def replace_tags(tags_list):\n",
        "    tag_mapping = {\n",
        "        0: \"B-company\",\n",
        "        1: \"B-TVshow\",\n",
        "        2: \"B-event\",\n",
        "        3: \"B-group\",\n",
        "        4: \"B-location\",\n",
        "        5: \"B-person\",\n",
        "        6: \"B-product\",\n",
        "        7: \"I-company\",\n",
        "        8: \"I-TVShow\",\n",
        "        9: \"I-event\",\n",
        "        10: \"I-group\",\n",
        "        11: \"I-location\",\n",
        "        12: \"I-person\",\n",
        "        13: \"I-product\",\n",
        "        14: \"O\"\n",
        "    }\n",
        "    return [tag_mapping.get(tag, tag) for tag in tags_list]\n",
        "\n",
        "# Replace tag numbers with specified mappings\n",
        "replaced_data = [{**item, \"tags\": replace_tags(item[\"tags\"])} for item in data_list]\n",
        "\n",
        "# Convert the replaced data to a string\n",
        "replaced_data_string = '\\n'.join(json.dumps(item) for item in replaced_data)\n",
        "\n",
        "# Write the replaced data to a new file\n",
        "with open('dataset_2020.train.json', 'w') as file:\n",
        "    file.write(replaced_data_string)\n"
      ],
      "metadata": {
        "id": "H9r0gbc9rpUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for converting json file to csv"
      ],
      "metadata": {
        "id": "OS-jgJ6D1Amy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Read the data from the input file\n",
        "with open('dataset_2020.train.json', 'r') as file:\n",
        "    data_lines = file.readlines()\n",
        "\n",
        "# Parse the JSON data\n",
        "data_list = [json.loads(line) for line in data_lines]\n",
        "\n",
        "# Create a DataFrame with 'tokens' and 'tags' columns\n",
        "df = pd.DataFrame(data_list, columns=['tokens', 'tags'])\n",
        "\n",
        "# Write the DataFrame to a CSV file\n",
        "df.to_csv('DataSet3.csv', index=False)\n"
      ],
      "metadata": {
        "id": "58MVdzKgtYMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jsonl to Json converter"
      ],
      "metadata": {
        "id": "HVyDVK9Yx4Zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Your original dataset\n",
        "with open('test.json', 'r') as file:\n",
        "    original_data = file.readlines()\n",
        "\n",
        "# Parse the JSON strings into dictionaries\n",
        "original_data = [json.loads(record) for record in original_data]\n",
        "\n",
        "# New dataset with only \"text\"\n",
        "new_data = [{\"text\": record[\"text\"]} for record in original_data]\n",
        "\n",
        "# Write the new dataset to a JSON file\n",
        "with open('new_dataset.json', 'w', encoding='utf-8') as outfile:\n",
        "    json.dump(new_data, outfile, indent=2)\n",
        "\n",
        "print(\"New dataset created and saved to 'new_dataset.json'\")\n"
      ],
      "metadata": {
        "id": "GMniJahnx2fF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "df493b69-508f-402c-f60a-15267bcef24d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 2 column 1 (char 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6a624d22aa5e>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Parse the JSON strings into dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0moriginal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moriginal_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# New dataset with only \"text\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-6a624d22aa5e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Parse the JSON strings into dictionaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0moriginal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moriginal_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# New dataset with only \"text\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 2 column 1 (char 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove all the extra columns in test.json except for text"
      ],
      "metadata": {
        "id": "9-PLRBf1zrph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Your original dataset\n",
        "with open('test.json', 'r') as file:\n",
        "    original_data = json.load(file)\n",
        "\n",
        "# New dataset with only \"text\"\n",
        "new_data = [{\"text\": record[\"text\"]} for record in original_data]\n",
        "\n",
        "# Write the new dataset to a JSON file\n",
        "with open('new_dataset.json', 'w', encoding='utf-8') as outfile:\n",
        "    json.dump(new_data, outfile, indent=2)\n",
        "\n",
        "print(\"New dataset created and saved to 'new_dataset.json'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GciHGz-yzxJB",
        "outputId": "47810914-7821-435e-cbc2-74ef55017ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New dataset created and saved to 'new_dataset.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Your original dataset\n",
        "with open('new_dataset.json', 'r') as file:\n",
        "    original_data = json.load(file)\n",
        "\n",
        "# New dataset with only \"text\" in a single line\n",
        "new_data = [record[\"text\"] for record in original_data]\n",
        "\n",
        "# Write the new dataset to a JSON file\n",
        "with open('new_dataset.json', 'w', encoding='utf-8') as outfile:\n",
        "    for item in new_data:\n",
        "        outfile.write(item + '\\n')\n",
        "\n",
        "print(\"New dataset created and saved to 'new_dataset.txt'\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtj5JqkX00c-",
        "outputId": "92c3c87b-0526-44dc-fed5-4d8835d33ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New dataset created and saved to 'new_dataset.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manual annotation"
      ],
      "metadata": {
        "id": "pMvDz82z19Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Read the JSON file\n",
        "with open('new_dataset.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Process texts and store entities in two columns\n",
        "results = []\n",
        "\n",
        "for item in data:  # Remove [\"texts\"] here\n",
        "    text = item[\"text\"]\n",
        "    doc = nlp(text)\n",
        "\n",
        "    entities = [{'text': ent.text, 'label': ent.label_} for ent in doc.ents]\n",
        "\n",
        "    results.append({'text': text, 'entities': entities})\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Print or save the DataFrame\n",
        "df.to_csv('Dataset3.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "rxx4zS6I18br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Read the JSON file\n",
        "with open('new_dataset.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Define the entity types\n",
        "entity_types = {\n",
        "    \"B-company\",\n",
        "    \"B-TVShow\",\n",
        "    \"B-event\",\n",
        "    \"B-group\",\n",
        "    \"B-location\",\n",
        "    \"B-person\",\n",
        "    \"B-product\",\n",
        "    \"I-company\",\n",
        "    \"I-TVShow\",\n",
        "    \"I-event\",\n",
        "    \"I-group\",\n",
        "    \"I-location\",\n",
        "    \"I-person\",\n",
        "    \"I-product\",\n",
        "    \"O\"\n",
        "}\n",
        "\n",
        "# Process texts and store entities in two columns\n",
        "results = []\n",
        "\n",
        "for item in data:\n",
        "    text = item.get(\"text\", \"\")\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Initialize BIO tags for each token\n",
        "    bio_entities = ['O'] * len(doc)\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        entity_type = ent.text\n",
        "        if entity_type in entity_types:\n",
        "            start = ent.start\n",
        "            end = ent.end\n",
        "\n",
        "            # Set BIO tags for the entity\n",
        "            bio_entities[start] = entity_type\n",
        "            for i in range(start + 1, end):\n",
        "                bio_entities[i] = f\"I-{entity_type}\"\n",
        "\n",
        "    results.append({'text': text, 'entities': bio_entities})\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Print or save the DataFrame\n",
        "df.to_csv('Dataset3.csv', index=False)\n"
      ],
      "metadata": {
        "id": "5XmAjuVaNdLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Read data from a JSON file\n",
        "with open('new_dataset.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Process texts and store entities with BIO tags\n",
        "results = []\n",
        "\n",
        "for item in data:\n",
        "    text = item.get(\"text\", \"\")\n",
        "    doc = nlp(text)\n",
        "\n",
        "    entities = []\n",
        "    current_entity = None\n",
        "\n",
        "    for token in doc:\n",
        "        if token.ent_iob_ == 'B':\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "            current_entity = {'text': token.text, 'label': f'B-{token.ent_type_}'}\n",
        "        elif token.ent_iob_ == 'I':\n",
        "            if current_entity:\n",
        "                current_entity['text'] += ' ' + token.text\n",
        "        else:\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = None\n",
        "\n",
        "    results.append({'text': text, 'entities': entities})\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "df.to_csv('Dataset3.csv', index=False)\n",
        "\n",
        "print(\"Results saved to 'output_dataset.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-rbQ9HmVQiX",
        "outputId": "04006bc3-fa58-42f9-96bd-ed4e5fbda523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to 'output_dataset.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Load your JSON data\n",
        "with open('new_dataset.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Set up Label Studio API endpoint\n",
        "label_studio_url = 'https://app.labelstudio.ai/api'\n",
        "\n",
        "# Create a Label Studio project\n",
        "project_creation_payload = {\n",
        "    'data': data,  # Your JSON data\n",
        "    'config': {\n",
        "        'language': 'en',\n",
        "        'views': ['ner'],\n",
        "        'nerLabels': ['B-person', 'I-person', 'B-organization', 'I-organization', 'B-location', 'I-location',\"B-TVShow\",\"I-TVShow\",\"B-event\",\"I-event\",\"B-group\",\n",
        "    \"I-group\",\"B-Product\",\"I-Product\",\"B-GPE\",\"I-GPE\",\"O\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "response = requests.post(f'{label_studio_url}/projects', json=project_creation_payload)\n",
        "project_id = response.json()['id']\n",
        "\n",
        "# Open Label Studio in your web browser and annotate the data\n",
        "\n",
        "# Once annotation is done, retrieve the labeled data\n",
        "annotations_response = requests.get(f'{label_studio_url}/projects/{project_id}/annotations')\n",
        "annotations = annotations_response.json()\n",
        "\n",
        "# Convert the labeled data to a DataFrame\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for annotation in annotations:\n",
        "    text = annotation['data']['text']\n",
        "    entities = []\n",
        "\n",
        "    for label in annotation['result']['ner']['spans']:\n",
        "        entities.append({\n",
        "            'start': label['start'],\n",
        "            'end': label['end'],\n",
        "            'label': label['label']\n",
        "        })\n",
        "\n",
        "    df = pd.concat([df, pd.DataFrame({'text': [text], 'entities': [entities]})])\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('labeled_data.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "5X8b3InbmUu9",
        "outputId": "44f54f46-834e-49e9-d30c-d21947b9da4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionError",
          "evalue": "HTTPSConnectionPool(host='app.labelstudio.ai', port=443): Max retries exceeded with url: /api/projects (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7d8b34229e10>: Failed to resolve 'app.labelstudio.ai' ([Errno -2] Name or service not known)\"))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             sock = connection.create_connection(\n\u001b[0m\u001b[1;32m    204\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mgaierror\u001b[0m: [Errno -2] Name or service not known",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrap_proxy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0msock\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLSocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaierror\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNameResolutionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7d8b34229e10>: Failed to resolve 'app.labelstudio.ai' ([Errno -2] Name or service not known)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    846\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreason\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='app.labelstudio.ai', port=443): Max retries exceeded with url: /api/projects (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7d8b34229e10>: Failed to resolve 'app.labelstudio.ai' ([Errno -2] Name or service not known)\"))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-81e65f530530>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m }\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{label_studio_url}/projects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject_creation_payload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mproject_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \"\"\"\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='app.labelstudio.ai', port=443): Max retries exceeded with url: /api/projects (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7d8b34229e10>: Failed to resolve 'app.labelstudio.ai' ([Errno -2] Name or service not known)\"))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bio_tagging(text, entities):\n",
        "    # Initialize a list to store BIO tags for each word\n",
        "    bio_tags = ['O'] * len(text.split())\n",
        "\n",
        "    for entity in entities:\n",
        "        entity_tokens = entity['text'].split()\n",
        "        start_idx = entity['start']\n",
        "        end_idx = entity['end']\n",
        "\n",
        "        for i in range(start_idx, end_idx):\n",
        "            if i == start_idx:\n",
        "                bio_tags[i] = 'B-' + entity['label']  # Begin tag\n",
        "            else:\n",
        "                bio_tags[i] = 'I-' + entity['label']  # Inside tag\n",
        "\n",
        "    return bio_tags\n",
        "\n",
        "# Example usage\n",
        "text_example = \"John Doe works at XYZ Corporation in New York.\"\n",
        "entities_example = [\n",
        "    {'text': 'John Doe', 'start': 0, 'end': 2, 'label': 'PERSON'},\n",
        "    {'text': 'XYZ Corporation', 'start': 5, 'end': 7, 'label': 'ORGANIZATION'},\n",
        "    {'text': 'New York', 'start': 8, 'end': 9, 'label': 'LOCATION'}\n",
        "]\n",
        "\n",
        "bio_tags_example = bio_tagging(text_example, entities_example)\n",
        "\n",
        "# Print the results\n",
        "print(\"Text:\", text_example)\n",
        "print(\"BIO Tags:\", bio_tags_example)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYPRkMmUuAz0",
        "outputId": "2e69f4ac-3b75-42bf-cb8a-55dc0770a3fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: John Doe works at XYZ Corporation in New York.\n",
            "BIO Tags: ['B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'O', 'B-LOCATION']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def bio_tagging(text, entities):\n",
        "    bio_tags = ['O'] * len(text.split())\n",
        "\n",
        "    for entity in entities:\n",
        "        entity_tokens = entity['text'].split()\n",
        "        start_idx = entity['start']\n",
        "        end_idx = entity['end']\n",
        "\n",
        "        for i in range(start_idx, end_idx):\n",
        "            if i == start_idx:\n",
        "                bio_tags[i] = 'B-' + entity['label']  # Begin tag\n",
        "            else:\n",
        "                bio_tags[i] = 'I-' + entity['label']  # Inside tag\n",
        "\n",
        "    return bio_tags\n",
        "\n",
        "def process_json_file(json_file):\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    for entry in data:\n",
        "        text = entry.get('text', '')  # Use get to handle missing 'text' field\n",
        "        entities = entry.get('entities', [])  # Use get to handle missing 'entities' field\n",
        "\n",
        "        bio_tags = bio_tagging(text, entities)\n",
        "\n",
        "        # Print or store the results as needed\n",
        "        print(\"Text:\", text)\n",
        "        print(\"BIO Tags:\", bio_tags)\n",
        "        print(\"\\n\")\n",
        "\n",
        "# Example usage\n",
        "json_file_example = 'new_dataset.json'  # Replace with your JSON file\n",
        "process_json_file(json_file_example)\n"
      ],
      "metadata": {
        "id": "Z7Gq8qe7uRAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Define the entity label mapping\n",
        "entity_label_mapping = {\n",
        "    \"B-corporation\": 0,\n",
        "    \"B-creative_work\": 1,\n",
        "    \"B-event\": 2,\n",
        "    \"B-group\": 3,\n",
        "    \"B-location\": 4,\n",
        "    \"B-person\": 5,\n",
        "    \"B-product\": 6,\n",
        "    \"I-corporation\": 7,\n",
        "    \"I-creative_work\": 8,\n",
        "    \"I-event\": 9,\n",
        "    \"I-group\": 10,\n",
        "    \"I-location\": 11,\n",
        "    \"I-person\": 12,\n",
        "    \"I-product\": 13,\n",
        "    \"O\": 14\n",
        "}\n",
        "\n",
        "def process_json_file(json_file):\n",
        "    # Read JSON data from file\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    texts = []\n",
        "    bio_tags_list = []\n",
        "\n",
        "    # Process each entry in the JSON file\n",
        "    for entry in data:\n",
        "        text = entry.get('text', '')\n",
        "        entities = entry.get('entities', [])\n",
        "\n",
        "        # Perform bio-tagging or processing logic here\n",
        "        bio_tags = []\n",
        "\n",
        "        for token in text.split():\n",
        "            entity_label = next((entity['label'] for entity in entities if entity['text'] == token), 'O')\n",
        "            bio_tags.append(entity_label_mapping.get(entity_label, entity_label_mapping['O']))\n",
        "\n",
        "        # Append results to lists\n",
        "        texts.append(text)\n",
        "        bio_tags_list.append(bio_tags)\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame({'Text': texts, 'BIO_Tags': bio_tags_list})\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    csv_filename = 'output_results.csv'\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Results saved to {csv_filename}\")\n",
        "\n",
        "# Example usage\n",
        "json_file_example = 'test.json'  # Replace with your JSON file\n",
        "process_json_file(json_file_example)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "faLfYop2w62m",
        "outputId": "71794345-80fd-4b0f-b2ae-f3f331219c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-76576276d5bc>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mjson_file_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test.json'\u001b[0m  \u001b[0;31m# Replace with your JSON file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mprocess_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-56-76576276d5bc>\u001b[0m in \u001b[0;36mprocess_json_file\u001b[0;34m(json_file)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mentity_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mbio_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_label_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_label_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'O'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-76576276d5bc>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mentity_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mbio_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_label_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_label_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'O'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Define the entity label mapping\n",
        "entity_label_mapping = {\n",
        "    \"B-corporation\": 0,\n",
        "    \"B-creative_work\": 1,\n",
        "    \"B-event\": 2,\n",
        "    \"B-group\": 3,\n",
        "    \"B-location\": 4,\n",
        "    \"B-person\": 5,\n",
        "    \"B-product\": 6,\n",
        "    \"I-corporation\": 7,\n",
        "    \"I-creative_work\": 8,\n",
        "    \"I-event\": 9,\n",
        "    \"I-group\": 10,\n",
        "    \"I-location\": 11,\n",
        "    \"I-person\": 12,\n",
        "    \"I-product\": 13,\n",
        "    \"O\": 14\n",
        "}\n",
        "\n",
        "def process_json_file(json_file):\n",
        "    # Read JSON data from file\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    texts = []\n",
        "    bio_tags_list = []\n",
        "\n",
        "    # Process each entry in the JSON file\n",
        "    for entry in data:\n",
        "        text = entry.get('text', '')\n",
        "        entities = entry.get('entities', [])\n",
        "\n",
        "        # Perform bio-tagging or processing logic here\n",
        "        # For this example, assuming bio_tags is a list of labels for each token\n",
        "        bio_tags = []\n",
        "\n",
        "        for token in text.split():\n",
        "            entity_label = next((entity.get('label', 'O') for entity in entities if entity.get('text', '') == token), 'O')\n",
        "            bio_tags.append(entity_label_mapping.get(entity_label, entity_label_mapping['O']))\n",
        "\n",
        "        # Append results to lists\n",
        "        texts.append(text)\n",
        "        bio_tags_list.append(bio_tags)\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame({'Text': texts, 'BIO_Tags': bio_tags_list})\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    csv_filename = 'output_results.csv'\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Results saved to {csv_filename}\")\n",
        "\n",
        "# Example usage\n",
        "json_file_example = 'test.json'  # Replace with your JSON file\n",
        "process_json_file(json_file_example)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IvL8EL9xt3o",
        "outputId": "4d3f317f-8b5e-4e64-c652-845171abf829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to output_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Define the entity label mapping\n",
        "entity_label_mapping = {\n",
        "    \"B-corporation\": 0,\n",
        "    \"B-creative_work\": 1,\n",
        "    \"B-event\": 2,\n",
        "    \"B-group\": 3,\n",
        "    \"B-location\": 4,\n",
        "    \"B-person\": 5,\n",
        "    \"B-product\": 6,\n",
        "    \"I-corporation\": 7,\n",
        "    \"I-creative_work\": 8,\n",
        "    \"I-event\": 9,\n",
        "    \"I-group\": 10,\n",
        "    \"I-location\": 11,\n",
        "    \"I-person\": 12,\n",
        "    \"I-product\": 13,\n",
        "    \"O\": 14\n",
        "}\n",
        "\n",
        "def process_json_file(json_file):\n",
        "    # Read JSON data from file\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    texts = []\n",
        "    bio_tags_list = []\n",
        "\n",
        "    # Process each entry in the JSON file\n",
        "    for entry in data:\n",
        "        text = entry.get('text', '')\n",
        "        entities = entry.get('entities', [])\n",
        "\n",
        "        # Perform bio-tagging or processing logic here\n",
        "        bio_tags = ['O'] * len(text.split())\n",
        "\n",
        "        for entity in entities:\n",
        "            start_idx = entity.get('start', 0)\n",
        "            end_idx = entity.get('end', 0)\n",
        "            entity_label = entity.get('label', 'O')\n",
        "\n",
        "            for i in range(start_idx, end_idx):\n",
        "                if i == start_idx:\n",
        "                    bio_tags[i] = f'B-{entity_label}'  # Begin tag\n",
        "                else:\n",
        "                    bio_tags[i] = f'I-{entity_label}'  # Inside tag\n",
        "\n",
        "        # Append results to lists\n",
        "        texts.append(text)\n",
        "        bio_tags_list.append(bio_tags)\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame({'Text': texts, 'BIO_Tags': bio_tags_list})\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    csv_filename = 'output_results.csv'\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Results saved to {csv_filename}\")\n",
        "\n",
        "# Example usage\n",
        "json_file_example = 'test.json'  # Replace with your JSON file\n",
        "process_json_file(json_file_example)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSbPn0DVx6Nm",
        "outputId": "7c58fc98-aeec-4367-aa1e-2346213e33f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to output_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Define the entity label mapping\n",
        "entity_label_mapping = {\n",
        "    \"B-corporation\": 0,\n",
        "    \"B-creative_work\": 1,\n",
        "    \"B-event\": 2,\n",
        "    \"B-group\": 3,\n",
        "    \"B-location\": 4,\n",
        "    \"B-person\": 5,\n",
        "    \"B-product\": 6,\n",
        "    \"I-corporation\": 7,\n",
        "    \"I-creative_work\": 8,\n",
        "    \"I-event\": 9,\n",
        "    \"I-group\": 10,\n",
        "    \"I-location\": 11,\n",
        "    \"I-person\": 12,\n",
        "    \"I-product\": 13,\n",
        "    \"O\": 14\n",
        "}\n",
        "\n",
        "def process_json_file(json_file):\n",
        "    # Read JSON data from file\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    texts = []\n",
        "    bio_tags_list = []\n",
        "\n",
        "    # Process each entry in the JSON file\n",
        "    for entry in data:\n",
        "        text = entry.get('text', '')\n",
        "        entities = entry.get('entities', [])\n",
        "\n",
        "        # Perform bio-tagging or processing logic here\n",
        "        bio_tags = ['O'] * len(text.split())\n",
        "\n",
        "        for entity in entities:\n",
        "            start_idx = entity.get('start', 0)\n",
        "            end_idx = entity.get('end', 0)\n",
        "            entity_label = entity.get('label', 'O')\n",
        "\n",
        "            # Ensure indices are within the range of the text\n",
        "            start_idx = min(start_idx, len(bio_tags) - 1)\n",
        "            end_idx = min(end_idx, len(bio_tags))\n",
        "\n",
        "            for i in range(start_idx, end_idx):\n",
        "                if i == start_idx:\n",
        "                    bio_tags[i] = f'B-{entity_label}'  # Begin tag\n",
        "                else:\n",
        "                    bio_tags[i] = f'I-{entity_label}'  # Inside tag\n",
        "\n",
        "        # Append results to lists\n",
        "        texts.append(text)\n",
        "        bio_tags_list.append(bio_tags)\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame({'Text': texts, 'BIO_Tags': bio_tags_list})\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    csv_filename = 'DataSet3.csv'\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Results saved to {csv_filename}\")\n",
        "\n",
        "# Example usage\n",
        "json_file_example = 'test.json'  # Replace with your JSON file\n",
        "process_json_file(json_file_example)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMCliPrtyH6_",
        "outputId": "111f4e01-f399-45d8-b6a5-7525139177ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to DataSet3.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Define the entity label mapping\n",
        "entity_label_mapping = {\n",
        "    \"corporation\": \"B-corporation\",\n",
        "    \"creative_work\": \"B-creative_work\",\n",
        "    \"event\": \"B-event\",\n",
        "    \"group\": \"B-group\",\n",
        "    \"location\": \"B-location\",\n",
        "    \"person\": \"B-person\",\n",
        "    \"product\": \"B-product\",\n",
        "}\n",
        "\n",
        "def process_json_file(json_file):\n",
        "    # Read JSON data from file\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    texts = []\n",
        "    bio_tags_list = []\n",
        "\n",
        "    # Process each entry in the JSON file\n",
        "    for entry in data:\n",
        "        text_tokenized = entry.get('text_tokenized', [])\n",
        "        entities = entry.get('entities', [])\n",
        "\n",
        "        # Perform bio-tagging or processing logic here\n",
        "        # For this example, assuming bio_tags is a list of labels for each token\n",
        "        bio_tags = ['O'] * len(text_tokenized)\n",
        "\n",
        "        for entity in entities:\n",
        "            entity_type = entity['type']\n",
        "            entity_label = entity_label_mapping.get(entity_type, 'O')\n",
        "            entity_text = entity['entity']\n",
        "\n",
        "            # Split the entity text into words\n",
        "            entity_words = entity_text.split()\n",
        "\n",
        "            # Find the start index of the first word in the entity\n",
        "            try:\n",
        "                start_idx = text_tokenized.index(entity_words[0])\n",
        "            except ValueError:\n",
        "                continue  # Skip entities not found in text_tokenized\n",
        "\n",
        "            # Find the end index of the last word in the entity\n",
        "            end_idx = start_idx + len(entity_words)\n",
        "\n",
        "            # Update bio-tags for the entity\n",
        "            for i in range(start_idx, end_idx):\n",
        "                if i == start_idx:\n",
        "                    bio_tags[i] = entity_label  # Begin tag\n",
        "                else:\n",
        "                    bio_tags[i] = 'I-' + entity_label.split('-')[1]  # Inside tag\n",
        "\n",
        "        # Append results to lists\n",
        "        texts.append(entry.get('text', ''))\n",
        "        bio_tags_list.append(bio_tags)\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame({'Text': texts, 'BIO_Tags': bio_tags_list})\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    csv_filename = 'DataSet3.csv'\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Results saved to {csv_filename}\")\n",
        "\n",
        "# Example usage\n",
        "json_file_example = 'test.json'  # Replace with your JSON file\n",
        "process_json_file(json_file_example)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kums7mXBzNrN",
        "outputId": "52f8a63d-78b5-423a-ecdf-32b15866c46a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to DataSet3.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Define the entity label mapping\n",
        "entity_label_mapping = {\n",
        "    \"corporation\": \"B-corporation\",\n",
        "    \"creative_work\": \"B-creative_work\",\n",
        "    \"event\": \"B-event\",\n",
        "    \"group\": \"B-group\",\n",
        "    \"location\": \"B-location\",\n",
        "    \"person\": \"B-person\",\n",
        "    \"product\": \"B-product\",\n",
        "}\n",
        "\n",
        "def process_json_file(json_file):\n",
        "    # Read JSON data from file\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    texts = []\n",
        "    bio_tags_list = []\n",
        "\n",
        "    # Process each entry in the JSON file\n",
        "    for entry in data:\n",
        "        text_tokenized = entry.get('text_tokenized', [])\n",
        "        entities = entry.get('entities', [])\n",
        "\n",
        "        # Perform bio-tagging or processing logic here\n",
        "        # For this example, assuming bio_tags is a list of labels for each token\n",
        "        bio_tags = ['O'] * len(text_tokenized)\n",
        "\n",
        "        for entity in entities:\n",
        "            entity_type = entity['type']\n",
        "            entity_label = entity_label_mapping.get(entity_type, 'O')\n",
        "            entity_text = entity['entity']\n",
        "\n",
        "            # Split the entity text into words\n",
        "            entity_words = entity_text.split()\n",
        "\n",
        "            # Find the start index of the first word in the entity\n",
        "            try:\n",
        "                start_idx = text_tokenized.index(entity_words[0])\n",
        "            except ValueError:\n",
        "                continue  # Skip entities not found in text_tokenized\n",
        "\n",
        "            # Find the end index of the last word in the entity\n",
        "            end_idx = start_idx + len(entity_words)\n",
        "\n",
        "            # Update bio-tags for the entity\n",
        "            for i in range(start_idx, end_idx):\n",
        "                if i == start_idx:\n",
        "                    bio_tags[i] = entity_label  # Begin tag\n",
        "                else:\n",
        "                    bio_tags[i] = 'I-' + entity_label.split('-')[1]  # Inside tag\n",
        "\n",
        "        # Append results to lists\n",
        "        texts.append(entry.get('text', ''))\n",
        "        bio_tags_list.append(bio_tags)\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame({'Text': texts, 'BIO_Tags': bio_tags_list})\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    csv_filename = 'DataSet3.csv'\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Results saved to {csv_filename}\")\n",
        "\n",
        "# Example usage\n",
        "json_file_example = 'test.json'  # Replace with your JSON file\n",
        "process_json_file(json_file_example)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvDyUUby1i1v",
        "outputId": "3c74e02c-5120-43d6-ccfc-caf2f56ded6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to DataSet3.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('DataSet2.csv', encoding='ISO-8859-1')\n",
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "8k3BbBNUnZHj",
        "outputId": "68a57e90-4aad-46c7-d1d2-bc749b649b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   tokens  \\\n",
              "count                                                2807   \n",
              "unique                                               2806   \n",
              "top     ['I', 'just', 'earned', 'the', \"'\", 'The', 'Da...   \n",
              "freq                                                    2   \n",
              "\n",
              "                                                     tags  \n",
              "count                                                2807  \n",
              "unique                                               2770  \n",
              "top     ['O', 'O', 'O', 'O', 'O', 'B-TVshow', 'I-TVSho...  \n",
              "freq                                                    5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-09e8d1de-dfa0-481b-a807-793a574a8f18\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2807</td>\n",
              "      <td>2807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2806</td>\n",
              "      <td>2770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>['I', 'just', 'earned', 'the', \"'\", 'The', 'Da...</td>\n",
              "      <td>['O', 'O', 'O', 'O', 'O', 'B-TVshow', 'I-TVSho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09e8d1de-dfa0-481b-a807-793a574a8f18')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-09e8d1de-dfa0-481b-a807-793a574a8f18 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-09e8d1de-dfa0-481b-a807-793a574a8f18');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d8adaefd-bcc0-4b37-bba9-492350da0fd8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d8adaefd-bcc0-4b37-bba9-492350da0fd8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d8adaefd-bcc0-4b37-bba9-492350da0fd8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2806,\n          \"2\",\n          \"2807\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tags\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2770,\n          \"5\",\n          \"2807\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rens3s1CJiSv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}