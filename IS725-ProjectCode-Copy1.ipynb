{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "810a30f6-5dc9-4d6a-9da3-c286a516ef70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2807</td>\n",
       "      <td>2807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2806</td>\n",
       "      <td>2770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>['I', 'just', 'earned', 'the', \"'\", 'The', 'Da...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'B-TVshow', 'I-TVSho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tokens  \\\n",
       "count                                                2807   \n",
       "unique                                               2806   \n",
       "top     ['I', 'just', 'earned', 'the', \"'\", 'The', 'Da...   \n",
       "freq                                                    2   \n",
       "\n",
       "                                                     tags  \n",
       "count                                                2807  \n",
       "unique                                               2770  \n",
       "top     ['O', 'O', 'O', 'O', 'O', 'B-TVshow', 'I-TVSho...  \n",
       "freq                                                    5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('DataSet2.csv')\n",
    "\n",
    "# Generating for the DataFrame\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24fecc51-e11c-4ebe-b947-105dbb1865cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf2crf\n",
      "  Downloading tf2crf-0.1.33-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: tensorflow>=2.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from tf2crf) (2.16.1)\n",
      "Collecting tensorflow-addons>=0.8.2 (from tf2crf)\n",
      "  Downloading tensorflow_addons-0.23.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (0.37.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.1.0->tf2crf) (1.26.4)\n",
      "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons>=0.8.2->tf2crf)\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow>=2.1.0->tf2crf) (0.41.2)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow>=2.1.0->tf2crf) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow>=2.1.0->tf2crf) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow>=2.1.0->tf2crf) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.1.0->tf2crf) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.1.0->tf2crf) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.1.0->tf2crf) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.1.0->tf2crf) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow>=2.1.0->tf2crf) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow>=2.1.0->tf2crf) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow>=2.1.0->tf2crf) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow>=2.1.0->tf2crf) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow>=2.1.0->tf2crf) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow>=2.1.0->tf2crf) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow>=2.1.0->tf2crf) (0.1.0)\n",
      "Downloading tf2crf-0.1.33-py2.py3-none-any.whl (7.3 kB)\n",
      "Downloading tensorflow_addons-0.23.0-cp311-cp311-macosx_11_0_arm64.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: typeguard, tensorflow-addons, tf2crf\n",
      "Successfully installed tensorflow-addons-0.23.0 tf2crf-0.1.33 typeguard-2.13.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf2crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b019038-a0ce-4bc5-81b0-37b26ca253c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/anaconda3/lib/python3.11/site-packages (3.3.3)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/lib/python3.11/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.11/site-packages (from keras) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.11/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/lib/python3.11/site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.11/site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in /opt/anaconda3/lib/python3.11/site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from optree->keras) (4.9.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a71797-5df9-4706-aa09-b38898550d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/anaconda3/lib/python3.11/site-packages (2.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d729a5ec-4d7c-4c9f-9ef7-233b0004a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "65af4f4b-f53b-48dc-85eb-40837325aa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/anithach/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/anithach/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('DataSet2.csv')\n",
    "\n",
    "# Function to perform POS tagging on a sentence\n",
    "def pos_tagging(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    return [tag for _, tag in pos_tags]\n",
    "\n",
    "# Apply POS tagging to the 'tokens' column\n",
    "df['pos_tags'] = df['tokens'].apply(lambda x: [pos_tagging(sentence) for sentence in x])\n",
    "# It assumes that each cell in the 'tokens' column  contains a list of sentences. \n",
    "#The list iterates through each sentence in x and applies the pos_tagging function to it.\n",
    "\n",
    "# Saving the updated dataset to csv file\n",
    "df.to_csv('DataSet2_with_POS.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c93dcc-c8c2-4c1b-89d3-b72042b6faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRF model without feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5719bbdf-c07f-45d3-82cd-3582ea929eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.7759572716253779\n",
      "Recall:  0.8384800515236772\n",
      "F1 Score:  0.8002605362512951\n",
      "Accuracy:  0.8384800515236772\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    B-TVshow       0.11      0.02      0.03       313\n",
      "   B-company       0.27      0.09      0.14       329\n",
      "     B-event       0.22      0.08      0.12       445\n",
      "     B-group       0.36      0.11      0.17       445\n",
      "  B-location       0.18      0.02      0.04       265\n",
      "    B-person       0.55      0.39      0.46       953\n",
      "   B-product       0.59      0.38      0.46       360\n",
      "    I-TVShow       0.03      0.01      0.01       434\n",
      "   I-company       0.04      0.02      0.02       109\n",
      "     I-event       0.18      0.05      0.07       515\n",
      "     I-group       0.08      0.01      0.01       159\n",
      "  I-location       0.00      0.00      0.00       110\n",
      "    I-person       0.33      0.21      0.25       324\n",
      "   I-product       0.03      0.01      0.01       137\n",
      "           O       0.87      0.98      0.92     24603\n",
      "\n",
      "    accuracy                           0.84     29501\n",
      "   macro avg       0.26      0.16      0.18     29501\n",
      "weighted avg       0.78      0.84      0.80     29501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.metrics import classification_report  # Import directly from sklearn\n",
    "\n",
    "# Loading the dataset into pandas dataframe\n",
    "df = pd.read_csv('DataSet2.csv')\n",
    "\n",
    "# Convert tokens and tags from strings to lists\n",
    "df['tokens'] = df['tokens'].apply(eval)\n",
    "df['tags'] = df['tags'].apply(eval)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['tokens'], df['tags'], test_size=0.2, random_state=42)\n",
    "# Splits the tokens and tags into training and testing sets \n",
    "# with 20% of the data reserved for testing.\n",
    "\n",
    "# Initializing and training the CRF model by using a function from CRF module\n",
    "crf = CRF()\n",
    "crf.fit(X_train.tolist(), y_train.tolist())  # Ensuring data is in list format\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred = crf.predict(X_test.tolist())\n",
    "\n",
    "# Flatten the list of tags for y_test and y_pred to avoid hierachial or multiple tags for a tokens\n",
    "# and to compute overall metrics\n",
    "y_test_flat = [tag for sublist in y_test for tag in sublist]\n",
    "y_pred_flat = [tag for sublist in y_pred for tag in sublist]\n",
    "\n",
    "# Calculate and print precision, recall, F1-score, and accuracy using functions in metrics module\n",
    "#The \"weighted average\" calculates each metric as per its occurence, and is used  in imbalanced datasets where some classes are significantly more common than others.\n",
    "print(\"Precision: \", metrics.flat_precision_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "print(\"Recall: \", metrics.flat_recall_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "print(\"F1 Score: \", metrics.flat_f1_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "print(\"Accuracy: \", metrics.flat_accuracy_score(y_test, y_pred))  # Accuracy calculation\n",
    "\n",
    "report = classification_report(y_test_flat, y_pred_flat)\n",
    "print(\" Classification Report:\\n\", report) # This prints the parameters for each entity type in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd16b8d-df0c-4836-b300-c9f42a811622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRF Model with feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a542b3-7aed-4967-b90d-f0a24076e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80953c8f-ed15-42d5-9170-9d24cbeb9ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    B-TVshow       0.40      0.22      0.28        55\n",
      "   B-company       0.33      0.12      0.17        59\n",
      "     B-event       0.56      0.14      0.22        71\n",
      "     B-group       0.69      0.37      0.48        59\n",
      "  B-location       0.58      0.28      0.38        39\n",
      "    B-person       0.68      0.61      0.64       137\n",
      "   B-product       0.69      0.46      0.55        63\n",
      "    I-TVShow       0.51      0.30      0.38        74\n",
      "   I-company       0.06      0.03      0.04        34\n",
      "     I-event       0.59      0.17      0.27        58\n",
      "     I-group       0.43      0.11      0.18        27\n",
      "  I-location       0.41      0.30      0.35        23\n",
      "    I-person       0.44      0.53      0.48        40\n",
      "   I-product       0.00      0.00      0.00        11\n",
      "           O       0.90      0.98      0.94      3784\n",
      "\n",
      "    accuracy                           0.87      4534\n",
      "   macro avg       0.48      0.31      0.36      4534\n",
      "weighted avg       0.84      0.87      0.85      4534\n",
      "\n",
      "Precision: 0.84\n",
      "Recall: 0.87\n",
      "Accuracy: 0.87\n",
      "F1-score: 0.85\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import ast\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv('DataSet1_with_POS.csv')\n",
    "df['tokens'] = df['tokens'].apply(ast.literal_eval)\n",
    "df['tags'] = df['tags'].apply(ast.literal_eval)\n",
    "\n",
    "def wordstofeatures(sent, i):\n",
    "# Defining a function to extract features from a word in a sentence.\n",
    "    word = sent[i] # Storing the word at position i in the sentence.\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:], # The last three characters of the word.\n",
    "        'word[-2:]': word[-2:], # The last two characters of the word.\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    # Contextual features using neighboring words.\n",
    "    if i > 0:\n",
    "        features.update({\n",
    "            '-1:word.lower()': sent[i-1].lower(), # The previous word in lowercase.\n",
    "            '-1:word.istitle()': sent[i-1].istitle(),\n",
    "        })\n",
    "    if i < len(sent) - 1:\n",
    "        features.update({\n",
    "            '+1:word.lower()': sent[i+1].lower(),\n",
    "            '+1:word.istitle()': sent[i+1].istitle(),\n",
    "        })\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [wordstofeatures(sent, i) for i in range(len(sent))]\n",
    "# Applying feature extraction to each word in the sentence by using wordstofeatures function.\n",
    "    # The function iterates over each word by its index,\n",
    "    # extracts relevant features using the word2features function,\n",
    "    # and compiles these into a list of feature dictionaries.\n",
    "\n",
    "X = df['tokens'].apply(sent2features).tolist()\n",
    "# Converting the tokens to features for all sentences in the dataframe and and converting it to list format for the CRF model.\n",
    "y = df['tags'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "# Splitting the dataset into training and testing sets using a 25% data for testing.\n",
    "\n",
    "crf = CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=200, all_possible_transitions=True)\n",
    "# Initializing the CRF model with the 'lbfgs' optimization algorithm and regularization parameters.\n",
    "\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "# Generating a detailed classification report that includes precision, recall, F1-score for each tag.\n",
    "\n",
    "print(\" Classification Report:\\n\", classification_report(y_test_flat, y_pred_flat))\n",
    "print(\"Precision: {:.2f}\".format(report['weighted avg']['precision'])) # the format function rounds off the decimal points\n",
    "print(\"Recall: {:.2f}\".format(report['weighted avg']['recall']))\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"F1-score: {:.2f}\".format(report['weighted avg']['f1-score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e6dc64-7fcf-48ce-b961-4e99aad1d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c831261-f075-4471-9763-c7f5d82aa6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    B-TVshow       0.48      0.25      0.33       393\n",
      "   B-company       0.51      0.26      0.34       412\n",
      "     B-event       0.55      0.31      0.40       556\n",
      "     B-group       0.57      0.36      0.44       561\n",
      "  B-location       0.68      0.43      0.53       329\n",
      "    B-person       0.70      0.64      0.67      1185\n",
      "   B-product       0.73      0.50      0.59       443\n",
      "    I-TVShow       0.44      0.36      0.40       530\n",
      "   I-company       0.21      0.12      0.15       137\n",
      "     I-event       0.54      0.31      0.39       603\n",
      "     I-group       0.21      0.17      0.19       196\n",
      "  I-location       0.50      0.39      0.44       140\n",
      "    I-person       0.70      0.64      0.67       398\n",
      "   I-product       0.05      0.04      0.04       158\n",
      "           O       0.92      0.98      0.95     31006\n",
      "\n",
      "    accuracy                           0.88     37047\n",
      "   macro avg       0.52      0.38      0.43     37047\n",
      "weighted avg       0.86      0.88      0.87     37047\n",
      "\n",
      "Precision: 0.86\n",
      "Recall: 0.88\n",
      "Accuracy: 0.88\n",
      "F1-score: 0.87\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import ast\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv('DataSet2_with_POS.csv')\n",
    "df['tokens'] = df['tokens'].apply(ast.literal_eval)\n",
    "df['tags'] = df['tags'].apply(ast.literal_eval)\n",
    "\n",
    "def wordstofeatures(sent, i):\n",
    "# Defining a function to extract features from a word in a sentence.\n",
    "    word = sent[i] # Storing the word at position i in the sentence.\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:], # The last three characters of the word.\n",
    "        'word[-2:]': word[-2:], # The last two characters of the word.\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    # Contextual features using neighboring words.\n",
    "    if i > 0:\n",
    "        features.update({\n",
    "            '-1:word.lower()': sent[i-1].lower(), # The previous word in lowercase.\n",
    "            '-1:word.istitle()': sent[i-1].istitle(),\n",
    "        })\n",
    "    if i < len(sent) - 1:\n",
    "        features.update({\n",
    "            '+1:word.lower()': sent[i+1].lower(),\n",
    "            '+1:word.istitle()': sent[i+1].istitle(),\n",
    "        })\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "# Applying feature extraction to each word in the sentence by using wordstofeatures function.\n",
    "    # The function iterates over each word by its index,\n",
    "    # extracts relevant features using the word2features function,\n",
    "    # and compiles these into a list of feature dictionaries.\n",
    "\n",
    "X = df['tokens'].apply(sent2features).tolist()\n",
    "# Converting the tokens to features for all sentences in the dataframe and and converting it to list format for the CRF model.\n",
    "y = df['tags'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "# Splitting the dataset into training and testing sets using a 25% data for testing.\n",
    "\n",
    "crf = CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=200, all_possible_transitions=True)\n",
    "# Initializing the CRF model with the 'lbfgs' optimization algorithm and regularization parameters.\n",
    "\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "# Generating a detailed classification report that includes precision, recall, F1-score for each tag.\n",
    "\n",
    "print(\" Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Precision: {:.2f}\".format(report['weighted avg']['precision'])) # the format function rounds off the decimal points\n",
    "print(\"Recall: {:.2f}\".format(report['weighted avg']['recall']))\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"F1-score: {:.2f}\".format(report['weighted avg']['f1-score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0cad1-30aa-444a-8cb9-b776bf8f8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3e331-54f0-47c0-908d-18b655cb1653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b895e9e-079c-4d1f-aa07-5daa453df112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.5470 - loss: 2.2112 - val_accuracy: 0.7322 - val_loss: 0.8738\n",
      "Epoch 2/8\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8215 - loss: 0.7401 - val_accuracy: 0.9171 - val_loss: 0.4360\n",
      "Epoch 3/8\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9184 - loss: 0.4385 - val_accuracy: 0.9222 - val_loss: 0.3950\n",
      "Epoch 4/8\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9218 - loss: 0.4008 - val_accuracy: 0.9236 - val_loss: 0.3854\n",
      "Epoch 5/8\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9235 - loss: 0.3885 - val_accuracy: 0.9239 - val_loss: 0.3815\n",
      "Epoch 6/8\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9207 - loss: 0.3972 - val_accuracy: 0.9236 - val_loss: 0.3783\n",
      "Epoch 7/8\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.9204 - loss: 0.3909 - val_accuracy: 0.9239 - val_loss: 0.3749\n",
      "Epoch 8/8\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9197 - loss: 0.3833 - val_accuracy: 0.9239 - val_loss: 0.3715\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9239 - loss: 0.3812\n",
      "Loss: 0.39116591215133667, Accuracy: 0.922160267829895\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step\n",
      "Precision: 0.86\n",
      "Accuracy: 0.92\n",
      "Recall: 0.92\n",
      "F1 Score: 0.89\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Input, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import ast\n",
    "\n",
    "# Load and preprocess the data\n",
    "df = pd.read_csv('dataset1_with_POS.csv')\n",
    "df['tokens'] = df['tokens'].apply(ast.literal_eval)\n",
    "df['tags'] = df['tags'].apply(ast.literal_eval)\n",
    "\n",
    "# Creating a list of all sentences and the corresponding tags\n",
    "sentences = df['tokens'].tolist()\n",
    "tags = df['tags'].tolist()\n",
    "\n",
    "# Encoding words and tags\n",
    "wordenc = LabelEncoder() # Using a label encoder for the tokens to convert words to integers for model processing.\n",
    "tagenc = LabelEncoder()\n",
    "\n",
    "allwords = [word for sent in sentences for word in sent]  # Combining all words from all sentences into a single list for the encoder.\n",
    "alltags = [tag for taglist in tags for tag in taglist]\n",
    "\n",
    "wordenc.fit(allwords) # Training the label encoder on the list of words to create a mapping from word to unique integer in encoder.\n",
    "tagenc.fit(alltags)\n",
    "\n",
    "# Transform each sentence and tag list to encoded form\n",
    "X_enc = [wordenc.transform(sent) for sent in sentences] # Converting each sentence from a list of words to a list of integers based on the previously trained encoder.\n",
    "y_enc = [tagenc.transform(taglist) for taglist in tags]\n",
    "\n",
    "# Padding sequences\n",
    "max_len = max(len(x) for x in X_enc) # Determining the maximum length of any sentence for padding\n",
    "X_pad = pad_sequences(X_enc, maxlen=max_len, padding='post')\n",
    "y_pad = pad_sequences(y_enc, maxlen=max_len, padding='post')\n",
    "\n",
    "# Convert tag sequences to categorical\n",
    "y_cat = [to_categorical(i, num_classes=len(tagenc.classes_)) for i in y_pad] # Convert each padded tag sequence into a one-hot encoded format for classification\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, np.array(y_cat), test_size=0.1, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "input = Input(shape=(max_len,)) # Defining the input layer \n",
    "model = Embedding(input_dim=len(wordenc.classes_), output_dim=50)(input)  # Embedding layer to convert token indices to vectors.\n",
    "model = Bidirectional(LSTM(units=64, return_sequences=True, recurrent_dropout=0.1))(model)# Bidirectional LSTM for learning from both directions of sequences.\n",
    "out = TimeDistributed(Dense(len(tagenc.classes_), activation=\"softmax\"))(model)# TimeDistributed Dense layer to predict a tag for each token.\n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])# Compiling the model with optimizer, loss function, and evaluation metrics.\n",
    "\n",
    "# Training the model with 8 epochs\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=8, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Evaluating the model\n",
    "eval = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {eval[0]}, Accuracy: {eval[1]}')\n",
    "\n",
    "# Prediction\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Converting predictions and true labels to original format for evaluation\n",
    "y_pred_argmax = np.argmax(y_pred, axis=-1).flatten()\n",
    "y_test_argmax = np.argmax(y_test, axis=-1).flatten()\n",
    "\n",
    "# Calculating precision, recall, and F1-score\n",
    "accuracy = accuracy_score(y_test_argmax, y_pred_argmax)\n",
    "precision = precision_score(y_test_argmax, y_pred_argmax, average='weighted',zero_division=0)\n",
    "recall = recall_score(y_test_argmax, y_pred_argmax, average='weighted',zero_division=0)\n",
    "f1 = f1_score(y_test_argmax, y_pred_argmax, average='weighted',zero_division=0)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d2b239-3c39-4923-bc79-7d79237b56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c580934-2943-404e-bc38-972341d2a8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.7435 - loss: 1.2661 - val_accuracy: 0.9315 - val_loss: 0.3545\n",
      "Epoch 2/8\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9326 - loss: 0.3445 - val_accuracy: 0.9314 - val_loss: 0.3316\n",
      "Epoch 3/8\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.9325 - loss: 0.3155 - val_accuracy: 0.9315 - val_loss: 0.2957\n",
      "Epoch 4/8\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.9331 - loss: 0.2671 - val_accuracy: 0.9320 - val_loss: 0.2692\n",
      "Epoch 5/8\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.9347 - loss: 0.2272 - val_accuracy: 0.9341 - val_loss: 0.2611\n",
      "Epoch 6/8\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.9436 - loss: 0.1941 - val_accuracy: 0.9345 - val_loss: 0.2564\n",
      "Epoch 7/8\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.9484 - loss: 0.1804 - val_accuracy: 0.9360 - val_loss: 0.2621\n",
      "Epoch 8/8\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.9521 - loss: 0.1664 - val_accuracy: 0.9388 - val_loss: 0.2536\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9401 - loss: 0.2521 \n",
      "Loss: 0.2628682851791382, Accuracy: 0.9385218620300293\n",
      "WARNING:tensorflow:5 out of the last 18 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16108a700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
      "Precision: 0.92\n",
      "Accuracy: 0.94\n",
      "Recall: 0.94\n",
      "F1 Score: 0.92\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Input, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import ast\n",
    "\n",
    "# Load and preprocess the data\n",
    "df = pd.read_csv('dataset2_with_POS.csv')\n",
    "df['tokens'] = df['tokens'].apply(ast.literal_eval)\n",
    "df['tags'] = df['tags'].apply(ast.literal_eval)\n",
    "\n",
    "# Creating a list of all sentences and the corresponding tags\n",
    "sentences = df['tokens'].tolist()\n",
    "tags = df['tags'].tolist()\n",
    "\n",
    "# Encoding words and tags\n",
    "wordenc = LabelEncoder() # Using a label encoder for the tokens to convert words to integers for model processing.\n",
    "tagenc = LabelEncoder()\n",
    "\n",
    "allwords = [word for sent in sentences for word in sent]  # Combining all words from all sentences into a single list for the encoder.\n",
    "alltags = [tag for taglist in tags for tag in taglist]\n",
    "\n",
    "wordenc.fit(allwords) # Training the label encoder on the list of words to create a mapping from word to unique integer in encoder.\n",
    "tagenc.fit(alltags)\n",
    "\n",
    "# Transform each sentence and tag list to encoded form\n",
    "X_enc = [wordenc.transform(sent) for sent in sentences] # Converting each sentence from a list of words to a list of integers based on the previously trained encoder.\n",
    "y_enc = [tagenc.transform(taglist) for taglist in tags]\n",
    "\n",
    "# Padding sequences\n",
    "max_len = max(len(x) for x in X_enc) # Determining the maximum length of any sentence for padding\n",
    "X_pad = pad_sequences(X_enc, maxlen=max_len, padding='post')\n",
    "y_pad = pad_sequences(y_enc, maxlen=max_len, padding='post')\n",
    "\n",
    "# Convert tag sequences to categorical\n",
    "y_cat = [to_categorical(i, num_classes=len(tagenc.classes_)) for i in y_pad] # Convert each padded tag sequence into a one-hot encoded format for classification\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, np.array(y_cat), test_size=0.1, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "input = Input(shape=(max_len,)) # Defining the input layer \n",
    "model = Embedding(input_dim=len(wordenc.classes_), output_dim=50)(input)  # Embedding layer to convert token indices to vectors.\n",
    "model = Bidirectional(LSTM(units=64, return_sequences=True, recurrent_dropout=0.1))(model)# Bidirectional LSTM for learning from both directions of sequences.\n",
    "out = TimeDistributed(Dense(len(tagenc.classes_), activation=\"softmax\"))(model)# TimeDistributed Dense layer to predict a tag for each token.\n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])# Compiling the model with optimizer, loss function, and evaluation metrics.\n",
    "\n",
    "# Training the model with 8 epochs\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=8, validation_split=0.1, verbose=1) # Training the model\n",
    "\n",
    "# Evaluating the model\n",
    "eval = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {eval[0]}, Accuracy: {eval[1]}')\n",
    "\n",
    "# Prediction\n",
    "y_pred = model.predict(X_test)  # Print the loss and accuracy obtained on the test set.\n",
    "\n",
    "# Converting predictions and true labels to original format for evaluation\n",
    "y_pred_argmax = np.argmax(y_pred, axis=-1).flatten()\n",
    "y_test_argmax = np.argmax(y_test, axis=-1).flatten()\n",
    "\n",
    "# Calculating precision, recall, and F1-score\n",
    "accuracy = accuracy_score(y_test_argmax, y_pred_argmax)\n",
    "precision = precision_score(y_test_argmax, y_pred_argmax, average='weighted',zero_division=0)\n",
    "recall = recall_score(y_test_argmax, y_pred_argmax, average='weighted',zero_division=0)\n",
    "f1 = f1_score(y_test_argmax, y_pred_argmax, average='weighted',zero_division=0)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d32cd9-9855-4387-a198-9cb5c72ee86c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
